{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from functools import partial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from mist import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mist.utils.plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([\"seaborn-white\", \"seaborn-paper\"])\n",
    "plt.rc(\"font\", family=\"sans serif\")\n",
    "# sns.set_palette(['#9e0059', '#6da7de', '#ee266d', '#dee000', '#eb861e'])\n",
    "sns.set_context(\"paper\", font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_POOL_TUPLES = [(\"Cosine\", \"spectra\"), (\"LL\", \"spectra\"), (\"LL\", \"bit\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll(pred, targ):\n",
    "    \"\"\"nll.\n",
    "\n",
    "    Args:\n",
    "        pred:\n",
    "        targ:\n",
    "    \"\"\"\n",
    "    log = partial(utils.clamped_log_np, _min=-5)\n",
    "    ll = targ * log(pred) + (1 - targ) * log(1 - pred)\n",
    "    return ll\n",
    "\n",
    "\n",
    "def cos_sim(pred, targ):\n",
    "    \"\"\"nll.\n",
    "\n",
    "    Args:\n",
    "        pred:\n",
    "        targ:\n",
    "    \"\"\"\n",
    "    pred\n",
    "\n",
    "    sim = cosine_similarity(pred, targ)\n",
    "    sim = np.diag(sim)\n",
    "    return sim[:, None]\n",
    "\n",
    "\n",
    "def tani(pred, targ):\n",
    "    \"\"\"tani.\n",
    "\n",
    "    Args:\n",
    "        pred:\n",
    "        targ:\n",
    "    \"\"\"\n",
    "    pred = np.copy(pred)\n",
    "    above_thresh = pred >= 0.5\n",
    "    pred[above_thresh] = 1.0\n",
    "    pred[~above_thresh] = 0.0\n",
    "\n",
    "    pred, targ = pred.astype(bool), targ.astype(bool)\n",
    "    denom = np.logical_or(pred, targ).sum(-1)\n",
    "    num = np.logical_and(pred, targ).sum(-1)\n",
    "    res = num / denom\n",
    "    return res[:, None]\n",
    "\n",
    "\n",
    "def get_metric(metric):\n",
    "    \"\"\"get_metric.\n",
    "\n",
    "    Args:\n",
    "        metric:\n",
    "    \"\"\"\n",
    "    \"\"\"get_metric.\n",
    "\n",
    "    Args:\n",
    "        metric:\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"LL\": ll,\n",
    "        \"Cosine\": cos_sim,\n",
    "        \"Tani\": tani,\n",
    "    }[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fp_results(\n",
    "    pred_file, baseline_file, model_name, baseline_name, metric, pool_method\n",
    "):\n",
    "    # Get preds and sort\n",
    "    fp_preds = pickle.load(open(pred_file, \"rb\"))\n",
    "    a_names, a_preds, a_targs = (\n",
    "        fp_preds[\"names\"],\n",
    "        fp_preds[\"preds\"],\n",
    "        fp_preds[\"targs\"],\n",
    "    )\n",
    "    a_names, a_preds, a_targs = (\n",
    "        np.array(a_names),\n",
    "        np.asarray(a_preds),\n",
    "        np.asarray(a_targs),\n",
    "    )\n",
    "    # print(a_preds[0], a_targs[0])\n",
    "    # a_names = np.array(a_names)\n",
    "    a_keep_set = set(a_names)\n",
    "\n",
    "    # Get baselines\n",
    "    b_preds = pickle.load(open(baseline_file, \"rb\"))\n",
    "    b_names, b_preds, b_targs = (\n",
    "        b_preds[\"names\"],\n",
    "        b_preds[\"preds\"],\n",
    "        b_preds[\"targs\"],\n",
    "    )\n",
    "    b_names, b_preds, b_targs = (\n",
    "        np.array(b_names),\n",
    "        np.array(b_preds),\n",
    "        np.array(b_targs),\n",
    "    )\n",
    "    b_keep_set = set(b_names)\n",
    "\n",
    "    # Get set overlap of names\n",
    "    keep_set = b_keep_set.intersection(a_keep_set)\n",
    "\n",
    "    # Filter both down to overlap\n",
    "    b_keep = [i in keep_set for i in b_names]\n",
    "    b_names, b_preds, b_targs = (\n",
    "        b_names[b_keep],\n",
    "        b_preds[b_keep],\n",
    "        b_targs[b_keep],\n",
    "    )\n",
    "\n",
    "    a_keep = [i in keep_set for i in a_names]\n",
    "    a_names, a_preds, a_targs = (\n",
    "        a_names[a_keep],\n",
    "        a_preds[a_keep],\n",
    "        a_targs[a_keep],\n",
    "    )\n",
    "\n",
    "    a_sort = np.argsort(a_names)\n",
    "    b_sort = np.argsort(b_names)\n",
    "    a_names, a_preds, a_targs = (\n",
    "        a_names[a_sort],\n",
    "        a_preds[a_sort],\n",
    "        a_targs[a_sort],\n",
    "    )\n",
    "    b_names, b_preds, b_targs = (\n",
    "        b_names[b_sort],\n",
    "        b_preds[b_sort],\n",
    "        b_targs[b_sort],\n",
    "    )\n",
    "\n",
    "    print(a_targs.shape, b_targs.shape)\n",
    "\n",
    "    assert np.all(a_targs == b_targs)\n",
    "\n",
    "    # Next compute matrix predictions\n",
    "    # Start with likelihoods\n",
    "    val_fn = get_metric(metric)\n",
    "    a_res = val_fn(a_preds, a_targs)\n",
    "    b_res = val_fn(b_preds, b_targs)\n",
    "\n",
    "    print(f\"{model_name} Mean values: {a_res.mean()}\")\n",
    "    print(f\"{baseline_name} Mean values: {b_res.mean()}\")\n",
    "\n",
    "    if pool_method == \"spectra\":\n",
    "        a_res = a_res.mean(1)\n",
    "        b_res = b_res.mean(1)\n",
    "    elif pool_method == \"bit\":\n",
    "        a_res = a_res.mean(0)\n",
    "        b_res = b_res.mean(0)\n",
    "\n",
    "    return a_res, b_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results_df(pred_files, metric, pool_method, out_dir):\n",
    "    # Get dataset names\n",
    "    datasets = list(set([p[2] for p in pred_files]))\n",
    "    datasets.sort()\n",
    "    val_fn = get_metric(metric)\n",
    "\n",
    "    result_df = pd.DataFrame(columns=[\"score\", \"model\", \"dataset\"])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"current dataset: {dataset}\")\n",
    "\n",
    "        dataset_preds = [p for p in pred_files if p[2] == dataset]\n",
    "        keep_sets = []\n",
    "\n",
    "        all_names, all_preds, all_targs, all_res = [], [], [], []\n",
    "\n",
    "        for fp_pred_file, _, _ in dataset_preds:\n",
    "\n",
    "            fp_preds = pickle.load(open(fp_pred_file, \"rb\"))\n",
    "\n",
    "            names, preds, targs = (\n",
    "                np.array(fp_preds[\"names\"]),\n",
    "                fp_preds[\"preds\"],\n",
    "                fp_preds[\"targs\"],\n",
    "            )\n",
    "            _, unique_idx = np.unique(names, return_index=True)\n",
    "\n",
    "            all_names.append(names[unique_idx])\n",
    "            all_preds.append(preds[unique_idx])\n",
    "            all_targs.append(targs[unique_idx])\n",
    "\n",
    "            keep_sets.append(list(set(names)))\n",
    "            print(f\"number of names: {len(names)}\")\n",
    "\n",
    "        keep_set = set.intersection(*map(set, keep_sets))\n",
    "\n",
    "        for i in range(len(all_names)):\n",
    "            keep = np.asarray(\n",
    "                [n in keep_set for n in all_names[i]], dtype=bool\n",
    "            )\n",
    "            all_names[i], all_preds[i], all_targs[i] = (\n",
    "                all_names[i][keep],\n",
    "                all_preds[i][keep],\n",
    "                all_targs[i][keep],\n",
    "            )\n",
    "            sort = np.argsort(all_names[i])\n",
    "            all_names[i], all_preds[i], all_targs[i] = (\n",
    "                all_names[i][sort],\n",
    "                all_preds[i][sort],\n",
    "                all_targs[i][sort],\n",
    "            )\n",
    "\n",
    "            res = val_fn(all_preds[i], all_targs[i])\n",
    "            print(f\"Mean values 1: {res.mean()}\")\n",
    "            if pool_method == \"spectra\":\n",
    "                res = res.mean(1)\n",
    "            elif pool_method == \"bit\":\n",
    "                res = res.mean(0)\n",
    "            all_res.append(res)\n",
    "            print(f\"shape results: {res.shape}\")\n",
    "        assert all(\n",
    "            [\n",
    "                np.all(all_targs[i] == all_targs[0])\n",
    "                for i in range(len(all_targs))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i, (_, model_name, dataset) in enumerate(dataset_preds):\n",
    "            res = pd.DataFrame(all_res[i], columns=[\"score\"])\n",
    "            res[\"model\"] = model_name\n",
    "            res[\"dataset\"] = dataset\n",
    "            result_df = pd.concat([result_df, res], ignore_index=True)\n",
    "\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    save_file = out_dir / f\"result_df_{metric}_{pool_method}.p\"\n",
    "    with open(save_file, \"wb\") as f:\n",
    "        pickle.dump(result_df, f)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_pred_file = \"../results/retrained_models/mist_fp_model_aug/preds_casmi2022/merged_fp_preds.p\"\n",
    "fp_base_file = \"../pretrained_models/mist_canopus_public/fp_model/preds_casmi2022/merged_fp_preds.p\"\n",
    "fp_model_name = \"MIST retrain\"\n",
    "fp_base_name = \"MIST public\"\n",
    "\n",
    "save_name = \"../../results/reproducibility/fp_scatter/scatter_plots.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_and_violin(\n",
    "    fp_pred_file,\n",
    "    fp_base_file,\n",
    "    fp_model_name,\n",
    "    fp_base_name,\n",
    "    violin_pred_files,\n",
    "    save_name,\n",
    "):\n",
    "    width = 7\n",
    "    height = width / 1.618\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(width * 2.2, height * 2.7))\n",
    "    scatter_axes = axes[0, :]\n",
    "    violin_axes = axes[1, :]\n",
    "    fig.tight_layout(h_pad=2)\n",
    "\n",
    "    # Scatter plots\n",
    "    for i, (ax, (metric, pool_method)) in enumerate(\n",
    "        zip(scatter_axes, METRIC_POOL_TUPLES)\n",
    "    ):\n",
    "        a_res, b_res = compute_fp_results(\n",
    "            fp_pred_file,\n",
    "            fp_base_file,\n",
    "            fp_model_name,\n",
    "            fp_base_name,\n",
    "            metric,\n",
    "            pool_method,\n",
    "        )\n",
    "\n",
    "        ax.set_box_aspect(1)\n",
    "\n",
    "        # set plot title\n",
    "        metric_title = {\n",
    "            \"LL\": \"Log Likelihood\",\n",
    "            \"Cosine\": \"Cosine Similarity\",\n",
    "            \"Tani\": \"Tanimoto Similarity\",\n",
    "        }.get(metric)\n",
    "        pool_title = {\"bit\": \"(fingerprint bits)\", \"spectra\": \"(spectra)\"}.get(\n",
    "            pool_method\n",
    "        )\n",
    "        top_title = f\"{metric_title}\\n{pool_title}\"\n",
    "        ax.set_title(top_title)\n",
    "\n",
    "        sns.kdeplot(\n",
    "            x=a_res.squeeze(),\n",
    "            y=b_res.squeeze(),\n",
    "            fill=True,\n",
    "            bw_adjust=0.6,\n",
    "            cmap=\"Oranges\",\n",
    "            ax=ax,\n",
    "            cbar=False,\n",
    "            cut=0,\n",
    "            levels=8,\n",
    "        )\n",
    "\n",
    "        a_higher = (a_res - b_res) > 0\n",
    "        b_higher = (b_res - a_res) > 0\n",
    "        num_a_higher = a_higher.sum()\n",
    "        num_b_higher = b_higher.sum()\n",
    "        c1 = [\"red\", \"red\"]\n",
    "        ax.scatter(\n",
    "            a_res[a_higher],\n",
    "            b_res[a_higher],\n",
    "            marker=\"X\",\n",
    "            s=0.2,\n",
    "            alpha=0.2,  # 0.2,\n",
    "            # c=c1[0],\n",
    "            c=\"orange\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            a_res[b_higher],\n",
    "            b_res[b_higher],\n",
    "            marker=\"X\",\n",
    "            s=0.2,\n",
    "            alpha=0.2,  # 0.2,\n",
    "            # c=c1[1],\n",
    "            c=\"orange\",\n",
    "        )\n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        lb = np.min([xlim[0], ylim[0]])\n",
    "        ub = np.max([xlim[1], ylim[1]])\n",
    "\n",
    "        ax.plot(\n",
    "            np.linspace(-100, 100, 100),\n",
    "            np.linspace(-100, 100, 100),\n",
    "            c=\"black\",\n",
    "            linewidth=0.7,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "        # Subset to 95th percentile for zoom\n",
    "        min_el = [min(i, j) for i, j in zip(a_res.squeeze(), b_res.squeeze())]\n",
    "        max_el = [min(i, j) for i, j in zip(a_res.squeeze(), b_res.squeeze())]\n",
    "\n",
    "        lb, ub = np.percentile(min_el, 1), np.percentile(max_el, 99)\n",
    "\n",
    "        if metric == \"LL\":\n",
    "            ax.set_xlim([lb, 0])\n",
    "            ax.set_ylim([lb, 0])\n",
    "        elif metric == \"Cosine\":\n",
    "            ax.set_xlim([0, 1])\n",
    "            ax.set_ylim([0, 1])\n",
    "        elif metric == \"Tani\":\n",
    "            ax.set_xlim([0, 1])\n",
    "            ax.set_ylim([0, 1])\n",
    "\n",
    "        ax.set_xlabel(f\"{fp_model_name}\")\n",
    "\n",
    "        print(f\"{fp_model_name} is higher: {num_a_higher}\")\n",
    "        print(f\"{fp_base_name} is higher: {num_b_higher}\")\n",
    "        left, width = 0, 0.5  # .25, 0.25 #0.5\n",
    "        bottom, height = 0.1, 0.5\n",
    "        right = left + width\n",
    "        top = bottom + height\n",
    "\n",
    "        corr, _ = pearsonr(a_res, b_res)\n",
    "        ax.text(\n",
    "            right,\n",
    "            bottom,\n",
    "            f\"correlation = {corr : 0.2f}\".expandtabs(),\n",
    "            transform=ax.transAxes,\n",
    "            # fontsize=15,\n",
    "        )\n",
    "\n",
    "        ax.text(\n",
    "            -0.1,\n",
    "            1.1,\n",
    "            string.ascii_lowercase[i],\n",
    "            transform=ax.transAxes,\n",
    "            size=15,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "\n",
    "    scatter_axes[0].set_ylabel(f\"{fp_base_name}\")\n",
    "\n",
    "    # Violin plots\n",
    "    out_dir = Path(save_name).parent\n",
    "    result_dfs = []\n",
    "    for metric, pool_method in METRIC_POOL_TUPLES:\n",
    "        res = compute_results_df(\n",
    "            violin_pred_files, metric, pool_method, out_dir\n",
    "        )\n",
    "        result_dfs.append(res)\n",
    "\n",
    "    for i, ((metric, pool_method), res_df, ax) in enumerate(\n",
    "        zip(METRIC_POOL_TUPLES, result_dfs, violin_axes)\n",
    "    ):\n",
    "        metric_title = {\n",
    "            \"LL\": \"Log likelihood\",\n",
    "            \"Cosine\": \"Cosine similarity\",\n",
    "            \"Tani\": \"Tanimoto similarity\",\n",
    "        }.get(metric)\n",
    "        pool_to_title = {\n",
    "            \"bit\": \"(fingerprint bits)\",\n",
    "            \"spectra\": \"(spectra)\",\n",
    "        }.get(pool_method)\n",
    "        ax.set_title(f\"{metric_title}\\n{pool_to_title}\")\n",
    "\n",
    "        ax.set_box_aspect(1)\n",
    "\n",
    "        sns.violinplot(\n",
    "            data=res_df,\n",
    "            x=\"dataset\",\n",
    "            y=\"score\",\n",
    "            hue=\"model\",\n",
    "            cut=0,\n",
    "            palette=sns.color_palette(\"tab10\"),\n",
    "            # linewidth=1,\n",
    "            legend=False,\n",
    "            ax=ax,\n",
    "            order=[\"MIST\", \"CASMI 2022\"],\n",
    "        )\n",
    "\n",
    "        ax.get_legend().remove()\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(ylabel=None)\n",
    "\n",
    "        ax.text(\n",
    "            -0.1,\n",
    "            1.1,\n",
    "            string.ascii_lowercase[i + 3],\n",
    "            transform=ax.transAxes,\n",
    "            size=15,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "\n",
    "    violin_axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=len(result_dfs))\n",
    "\n",
    "    plt.savefig(\n",
    "        save_name, format=\"pdf\", bbox_inches=\"tight\", dpi=600, transparent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casmi scatter and violin plots\n",
    "fp_pred_file = \"../../results/retrained_models/mist_fp_model_aug/preds_casmi2022/Fold_100_0_preds.p\"\n",
    "fp_base_file = \"../../pretrained_models/mist_canopus_public/fp_model/preds_casmi2022/fp_preds_casmi2022.p\"\n",
    "fp_model_name = \"MIST retrain\"\n",
    "fp_base_name = \"MIST public\"\n",
    "\n",
    "violin_pred_files = [  # (filename, model, dataset)\n",
    "    (\n",
    "        f\"../../pretrained_models/mist_canopus_public/fp_model/preds_casmi2022/fp_preds_casmi2022.p\",\n",
    "        \"MIST public\",\n",
    "        \"CASMI 2022\",\n",
    "    ),\n",
    "    (\n",
    "        f\"../../results/retrained_models/mist_fp_model_aug/preds_casmi2022/Fold_100_0_preds.p\",\n",
    "        \"MIST retrain\",\n",
    "        \"CASMI 2022\",\n",
    "    ),\n",
    "    (\n",
    "        f\"../../pretrained_models/mist_full/fp_model/preds_casmi2022/fp_preds_casmi2022.p\",\n",
    "        \"MIST full\",\n",
    "        \"CASMI 2022\",\n",
    "    ),\n",
    "    (\n",
    "        f\"../../pretrained_models/mist_canopus_public/fp_model/preds_canopus_train_public/fp_preds_canopus_train_public.p\",\n",
    "        \"MIST public\",\n",
    "        \"MIST\",\n",
    "    ),\n",
    "    (\n",
    "        f\"../../results/retrained_models/mist_fp_model_aug/preds_canopus_train_public/Fold_100_0_preds.p\",\n",
    "        \"MIST retrain\",\n",
    "        \"MIST\",\n",
    "    ),\n",
    "    (\n",
    "        f\"../../pretrained_models/mist_full/fp_model/preds_canopus_train_public/fp_preds_canopus_train_public.p\",\n",
    "        \"MIST full\",\n",
    "        \"MIST\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "save_name = \"../../results/reusability/scatter_violin.pdf\"\n",
    "\n",
    "plot_scatter_and_violin(\n",
    "    fp_pred_file,\n",
    "    fp_base_file,\n",
    "    fp_model_name,\n",
    "    fp_base_name,\n",
    "    violin_pred_files,\n",
    "    save_name,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
